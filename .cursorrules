# Role & Context
Tu es un développeur Senior Fullstack expert en TypeScript, Next.js (App Router) et Tailwind CSS. Tes réponses doivent être concises, performantes et privilégier la maintenabilité du code.

# Tech Stack & Preferences
- Frontend: React 18+, Next.js (App Router), Tailwind CSS.
- Backend: Next.js Server Actions, Prisma ORM, PostgreSQL.
- State Management: React Context ou TanStack Query (pas de Redux sauf demande explicite).
- Icons: Lucide-React.
- UI Components: Shadcn/UI.

# Coding Standards
- Utilise toujours TypeScript avec des interfaces claires. Évite 'any'.
- Préfère les 'Arrow Functions' pour les composants fonctionnels.
- Utilise la déstructuration d'objets pour les props.
- Favorise les composants côté serveur (RSC) par défaut; n'utilise 'use client' que si nécessaire (hooks, events).
- Utilise des noms de variables explicites et en anglais.

# CSS & Style
- Utilise exclusivement Tailwind CSS.
- Respecte une approche Mobile-first.
- Évite les styles inline ou le CSS arbitraire (ex: h-[123px]) si une classe standard existe.

# Error Prevention & Best Practices
- Vérifie toujours la présence de 'null' ou 'undefined' (Optional chaining).
- Inclus systématiquement la gestion d'erreurs (try/catch) dans les Server Actions.
- Utilise 'zod' pour la validation des données entrantes (schémas de formulaires ou API).
- Si une modification impacte plusieurs fichiers, préviens-moi avant de générer tout le code.

# Communication
- Réponds en français.
- Sois direct. Pas de politesses superflues, concentre-toi sur le code et l'explication technique.


- **Interdiction:** Ne génère plus de fichiers .md de diagnostic. Si un test échoue, propose un log `console.error` ou un script de test temporaire en `.ts`.

# BACKEND MONITORING & ADMIN RULES

## Tech Stack & Context
- **Database:** **S3 + Athena (Extreme Budget Architecture)** - BANNIR les bases de données relationnelles coûteuses (RDS, Aurora, Supabase).
- **Data Storage:** Toutes les données volumineuses (holdings, filings) doivent être stockées en Parquet sur S3.
- **Query Engine:** Amazon Athena pour toutes les requêtes analytiques (corrélation, agrégations, jointures).
- **Execution:** Serverless Lambdas / Edge Functions.
- **Goal:** Full observability of background jobs (13F parsing, 10K/10Q parsing, CRONs) for admin dashboard.

## ⚠️ RÈGLE CRITIQUE: Bannir les DB Relationnelles Coûteuses
- **INTERDIT:** Utiliser RDS, Aurora, Supabase, ou toute base de données relationnelle payante pour les données volumineuses.
- **OBLIGATOIRE:** Utiliser S3 (Parquet) + Athena pour toutes les données analytiques (holdings, filings, diffs).
- **EXCEPTION:** Seulement pour les métadonnées légères (< 10K rows), on peut utiliser DynamoDB (PAY_PER_REQUEST).
- **Pattern:** Toutes les nouvelles données doivent être écrites directement en Parquet sur S3, jamais dans une DB relationnelle.

## Database Schema Standards (Supabase)
Always use these monitoring tables. Write code that interacts with them for ALL background tasks.

### 1. `file_processing_queue` - Tracks file parsing status
**Columns:**
- `id` (uuid, primary key, default: gen_random_uuid())
- `filename` (text, required) - Original filename or identifier
- `status` (text, required) - ENUM: 'PENDING' | 'PROCESSING' | 'COMPLETED' | 'FAILED'
- `doc_type` (text) - ENUM: '13F' | '10K' | '10Q' | 'RSS' | 'OTHER'
- `filing_id` (integer, nullable) - Reference to fund_filings.id if applicable
- `fund_id` (integer, nullable) - Reference to funds.id if applicable
- `retry_count` (integer, default: 0) - Number of retry attempts
- `max_retries` (integer, default: 3) - Maximum retry attempts before permanent failure
- `error_log` (text, nullable) - Last error message
- `metrics` (jsonb, nullable) - Validation metrics (e.g., {"rows_parsed": 150, "holdings_count": 45})
- `started_at` (timestamptz, nullable) - When processing started
- `completed_at` (timestamptz, nullable) - When processing completed/failed
- `created_at` (timestamptz, default: now())
- `updated_at` (timestamptz, default: now())

**Indexes:**
- `idx_file_processing_queue_status` on (status) WHERE status IN ('PENDING', 'PROCESSING')
- `idx_file_processing_queue_filing_id` on (filing_id) WHERE filing_id IS NOT NULL
- `idx_file_processing_queue_created_at` on (created_at DESC)

### 2. `cron_registry` - Tracks recurring jobs health
**Columns:**
- `id` (text, primary key) - Cron identifier (e.g., 'collector-sec-watcher', 'parser-13f')
- `is_active` (boolean, default: true) - If false, cron should abort immediately
- `last_status` (text) - ENUM: 'SUCCESS' | 'FAILED' | 'RUNNING' | null
- `last_run_at` (timestamptz, nullable) - Last execution timestamp
- `last_success_at` (timestamptz, nullable) - Last successful execution
- `last_error` (text, nullable) - Last error message
- `run_count` (integer, default: 0) - Total number of executions
- `success_count` (integer, default: 0) - Total successful executions
- `failure_count` (integer, default: 0) - Total failed executions
- `schedule_expression` (text, nullable) - EventBridge schedule (e.g., 'rate(5 minutes)')
- `next_run_at` (timestamptz, nullable) - Estimated next run time
- `created_at` (timestamptz, default: now())
- `updated_at` (timestamptz, default: now())

**Indexes:**
- `idx_cron_registry_is_active` on (is_active) WHERE is_active = true
- `idx_cron_registry_last_run_at` on (last_run_at DESC)

## Coding Rules for Services & Lambdas

### 1. Job/Queue Pattern (The "Wrapper" Rule)
**MANDATORY** for any parsing function (13F, 10K, RSS, etc.):

```typescript
// Example lifecycle
1. initJob(filename, docType, filingId?) -> returns jobId
2. startJob(jobId) -> Set status to 'PROCESSING', started_at = now()
3. Try/Catch: Wrap entire parsing logic
4. On Success: completeJob(jobId, metrics?) -> Set status to 'COMPLETED', completed_at = now()
5. On Failure: failJob(jobId, error, retryDelay?) -> Set status to 'FAILED', increment retry_count, save error_log
```

**Retry Logic:**
- If `retry_count < max_retries`: Schedule retry (via SQS delay or EventBridge schedule)
- If `retry_count >= max_retries`: Mark as permanently failed

### 2. Data Storage Pattern (S3 + Athena)
For ALL data storage operations, use S3 Parquet + Athena instead of relational databases.

**Import pattern:**
```typescript
import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';
import { AthenaClient, StartQueryExecutionCommand } from '@aws-sdk/client-athena';
```

**Writing data (Parquet to S3):**
```typescript
// Convert data to Parquet and upload to S3
const parquetBuffer = await convertToParquet(data, schema);
await s3Client.send(new PutObjectCommand({
  Bucket: 'data-lake-bucket',
  Key: `data/${tableName}/year=${year}/month=${month}/data.parquet`,
  Body: parquetBuffer,
}));
```

**Reading data - Performance & Cost Optimization:**

**⚠️ CRITICAL: Athena Optimization Rules**

1. **Small Lookups (1 row by ID)**: Use S3 direct read, NOT Athena
   - Athena has a 10MB minimum billing per query
   - For `getCompany(id)` or `getFund(id)`, use `findRowByIdInS3Parquet()` from `@/athena/s3-direct-read`
   - Faster AND cheaper than Athena for single-row lookups

2. **Cache for Frequent Queries**: Use Lambda cache for repeated queries
   - For small tables (< 10K rows) that change rarely (companies, funds)
   - Use `withCache()` from `@/athena/cache` with 5-minute TTL
   - Example: `getCompanyByTicker('AAPL')` should be cached if called multiple times

3. **Athena for Analytical Queries Only**:
   - Use Athena for: aggregations, JOINs, GROUP BY, filtering large datasets
   - DON'T use Athena for: single-row lookups, small tables (< 100 rows)

**Examples:**

```typescript
// ❌ BAD: Athena for single row lookup (wastes 10MB minimum)
const company = await executeAthenaQuery(`SELECT * FROM companies WHERE id = ${id} LIMIT 1`);

// ✅ GOOD: S3 direct read for single row
import { findRowByIdInS3Parquet } from '@/athena/s3-direct-read';
const company = await findRowByIdInS3Parquet('companies', id);

// ✅ GOOD: Cache for frequent lookups
import { withCache, CacheKeys } from '@/athena/cache';
const company = await withCache(
  CacheKeys.companyByTicker(ticker),
  () => getCompanyByTickerAthena(ticker),
  5 * 60 * 1000 // 5 minutes
);

// ✅ GOOD: Athena for analytical queries
const results = await executeAthenaQuery(`
  SELECT ticker, SUM(market_value) as total_value
  FROM fund_holdings
  WHERE fund_id = ${fundId}
  GROUP BY ticker
`);
```

4. **Batch Processing**: Group Athena queries when possible
   - Instead of 10 separate queries, use 1 query with WHERE IN or UNION

5. **Parquet Only**: Never use CSV for tables > 10K rows
   - Parquet is columnar and much more efficient for Athena

**Legacy Supabase (migration in progress):**
- Only use Supabase for reading existing data during migration.
- All new data must go to S3 Parquet.
- Gradually migrate existing tables using `scripts/migrate_to_s3_parquet.ts`.

### 3. CRON Jobs Pattern
**MANDATORY** for every CRON function:

```typescript
1. Check cron_registry: const cron = await getCronStatus('cron-id')
2. If !cron.is_active: Abort immediately, return { skipped: true, reason: 'inactive' }
3. Log start: await updateCronStatus('cron-id', { last_status: 'RUNNING', last_run_at: now() })
4. Try/Catch: Wrap entire cron logic
5. On Success: await updateCronStatus('cron-id', { last_status: 'SUCCESS', last_success_at: now(), run_count: +1, success_count: +1 })
6. On Failure: await updateCronStatus('cron-id', { last_status: 'FAILED', last_error: error.message, run_count: +1, failure_count: +1 })
```

### 4. Parsing Specifics (13F/10K)
- **Always** extract validation metrics (e.g., `{"rows_parsed": 150, "holdings_count": 45, "validation_errors": 0}`)
- Store metrics in `file_processing_queue.metrics` (JSONB column)
- This allows the dashboard to verify data integrity alongside the "COMPLETED" status

### 5. Error Logging
- **Always** log full error details in `error_log` (not just message)
- Include stack trace for backend errors
- Include context (filing_id, fund_id, filename) in error message
- Format: `[ERROR] ${timestamp}: ${error.message}\nStack: ${error.stack}\nContext: ${JSON.stringify(context)}`

### 6. Metrics Collection
For dashboard analytics, track:
- Processing duration (started_at to completed_at)
- Retry patterns (retry_count distribution)
- Success/failure rates by doc_type
- Most common errors (from error_log aggregation)