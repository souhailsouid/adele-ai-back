# ğŸš€ Plan de Migration : Supabase â†’ AWS

## ğŸ“Š Ã‰tat Actuel

### DonnÃ©es Supabase
- **Entreprises** : 8,191
- **Filings SEC** : 206,194
- **Filings 13F** : 262
- **Holdings 13F** : 5,772,225
- **Stockage** : ~3 GB (dÃ©passement plan FREE)
- **CoÃ»t actuel** : $25-30/mois (plan PRO)

### Stack AWS Existante
- âœ… **Lambda** : API, Collectors, Parsers
- âœ… **Cognito** : Authentification
- âœ… **S3** : Stockage fichiers
- âœ… **EventBridge** : Orchestration
- âœ… **SQS** : Queues pour lissage
- âš ï¸ **Athena** : MentionnÃ© mais pas encore utilisÃ©

---

## ğŸ¯ Architecture Cible AWS

### StratÃ©gie de Data-Tiering

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DONNÃ‰ES VOLUMINEUSES                 â”‚
â”‚              S3 (Parquet) + Athena                      â”‚
â”‚  - fund_holdings (5.7M rows)                            â”‚
â”‚  - company_filings (206K rows)                          â”‚
â”‚  - fund_filings (262 rows, mais fichiers XML lourds)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DONNÃ‰ES RELATIONNELLES                      â”‚
â”‚              RDS PostgreSQL (db.t3.micro)              â”‚
â”‚  - companies (8K rows)                                   â”‚
â”‚  - funds (petit volume)                                 â”‚
â”‚  - fund_holdings_diff (calculs)                         â”‚
â”‚  - earnings_calendar                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DONNÃ‰ES HAUTE FRÃ‰QUENCE                     â”‚
â”‚              DynamoDB                                    â”‚
â”‚  - signals (realtime)                                   â”‚
â”‚  - notifications (frÃ©quentes)                           â”‚
â”‚  - cron_registry (mÃ©tadonnÃ©es)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Plan d'Action DÃ©taillÃ©

### Phase 1 : PrÃ©paration (Semaine 1)

#### 1.1 Analyse et Inventaire
- [ ] **Audit des tables Supabase**
  ```bash
  # Script Ã  crÃ©er : scripts/audit_supabase_tables.ts
  # - Lister toutes les tables
  # - Compter les rows par table
  # - Estimer la taille par table
  # - Identifier les dÃ©pendances (foreign keys)
  ```

- [ ] **Mapping Tables â†’ Services AWS**
  | Table Supabase | Service AWS | Raison |
  |----------------|-------------|--------|
  | `fund_holdings` | S3 + Athena | 5.7M rows, requÃªtes analytiques |
  | `company_filings` | S3 + Athena | 206K rows, fichiers volumineux |
  | `fund_filings` | S3 + Athena | Fichiers XML bruts |
  | `companies` | RDS PostgreSQL | 8K rows, relations frÃ©quentes |
  | `funds` | RDS PostgreSQL | Petit volume, relations |
  | `fund_holdings_diff` | RDS PostgreSQL | Calculs relationnels |
  | `signals` | DynamoDB | Haute frÃ©quence, realtime |
  | `notifications` | DynamoDB | Haute frÃ©quence |
  | `cron_registry` | DynamoDB | MÃ©tadonnÃ©es lÃ©gÃ¨res |
  | `earnings_calendar` | RDS PostgreSQL | Relations avec companies |

#### 1.2 CrÃ©ation Infrastructure Terraform
- [ ] **RDS PostgreSQL**
  ```hcl
  # infra/terraform/rds.tf
  resource "aws_db_instance" "main" {
    identifier     = "${var.project}-${var.stage}-db"
    engine         = "postgres"
    engine_version = "15.4"
    instance_class = "db.t3.micro"  # $15/mois
    allocated_storage = 20  # GB
    storage_type   = "gp3"
    
    db_name  = "personamy"
    username = var.db_username
    password = var.db_password
    
    vpc_security_group_ids = [aws_security_group.rds.id]
    db_subnet_group_name   = aws_db_subnet_group.main.name
    
    backup_retention_period = 7
    skip_final_snapshot    = false
    final_snapshot_identifier = "${var.project}-${var.stage}-final-snapshot"
  }
  ```

- [ ] **S3 Buckets pour Data Lake**
  ```hcl
  # infra/terraform/s3-data-lake.tf
  resource "aws_s3_bucket" "data_lake" {
    bucket = "${var.project}-${var.stage}-data-lake"
  }
  
  # Structure :
  # s3://data-lake/
  #   â”œâ”€â”€ fund_holdings/
  #   â”‚   â””â”€â”€ year=2024/month=01/day=15/
  #   â”‚       â””â”€â”€ holdings.parquet
  #   â”œâ”€â”€ company_filings/
  #   â”‚   â””â”€â”€ year=2024/month=01/
  #   â”‚       â””â”€â”€ filings.parquet
  #   â””â”€â”€ fund_filings/
  #       â””â”€â”€ raw/  # Fichiers XML bruts
  ```

- [ ] **Athena Database & Tables**
  ```hcl
  # infra/terraform/athena.tf
  resource "aws_athena_database" "main" {
    name   = "${var.project}_${var.stage}"
    bucket = aws_s3_bucket.athena_results.bucket
  }
  
  # Tables externes (Hive format) pointant vers S3 Parquet
  ```

- [ ] **DynamoDB Tables**
  ```hcl
  # infra/terraform/dynamodb.tf
  resource "aws_dynamodb_table" "signals" {
    name           = "${var.project}-${var.stage}-signals"
    billing_mode   = "PAY_PER_REQUEST"  # Pas de provisioned capacity
    hash_key       = "id"
    range_key      = "timestamp"
    
    attribute {
      name = "id"
      type = "S"
    }
    attribute {
      name = "timestamp"
      type = "S"
    }
    
    # GSI pour requÃªtes par source/type
    global_secondary_index {
      name     = "source-timestamp-index"
      hash_key = "source"
      range_key = "timestamp"
    }
  }
  ```

#### 1.3 Scripts de Migration
- [ ] **Export Supabase â†’ S3 (Parquet)**
  ```typescript
  // scripts/migrate_holdings_to_s3.ts
  // 1. Exporter fund_holdings depuis Supabase
  // 2. Convertir en Parquet
  // 3. Upload vers S3 avec partitionnement (year/month/day)
  ```

- [ ] **Export Supabase â†’ RDS**
  ```typescript
  // scripts/migrate_companies_to_rds.ts
  // 1. Exporter companies, funds, earnings_calendar
  // 2. InsÃ©rer dans RDS PostgreSQL
  ```

- [ ] **Export Supabase â†’ DynamoDB**
  ```typescript
  // scripts/migrate_signals_to_dynamodb.ts
  // 1. Exporter signals, notifications
  // 2. Batch write vers DynamoDB
  ```

---

### Phase 2 : Migration des DonnÃ©es (Semaine 2)

#### 2.1 Migration S3 + Athena (Gros Volumes)

**Ã‰tape 1 : Export fund_holdings**
```bash
npx tsx scripts/migrate_holdings_to_s3.ts \
  --batch-size=100000 \
  --s3-bucket=personamy-prod-data-lake \
  --s3-prefix=fund_holdings/
```

**Ã‰tape 2 : CrÃ©er table Athena**
```sql
-- infra/terraform/athena-schemas/fund_holdings.sql
CREATE EXTERNAL TABLE fund_holdings (
  id BIGINT,
  fund_id INT,
  filing_id INT,
  ticker STRING,
  cusip STRING,
  shares BIGINT,
  market_value BIGINT,
  type STRING,
  created_at TIMESTAMP
)
PARTITIONED BY (year INT, month INT, day INT)
STORED AS PARQUET
LOCATION 's3://personamy-prod-data-lake/fund_holdings/'
TBLPROPERTIES ('parquet.compress'='SNAPPY');
```

**Ã‰tape 3 : Repartitionner les donnÃ©es existantes**
```sql
MSCK REPAIR TABLE fund_holdings;
```

#### 2.2 Migration RDS (DonnÃ©es Relationnelles)

**Ã‰tape 1 : CrÃ©er schÃ©ma RDS**
```bash
# Appliquer migrations SQL sur RDS
psql -h $RDS_ENDPOINT -U $DB_USER -d personamy -f infra/supabase/migrations/001_initial_schema.sql
psql -h $RDS_ENDPOINT -U $DB_USER -d personamy -f infra/supabase/migrations/003_add_companies_tables.sql
# ... autres migrations nÃ©cessaires
```

**Ã‰tape 2 : Migrer les donnÃ©es**
```bash
npx tsx scripts/migrate_companies_to_rds.ts \
  --supabase-url=$SUPABASE_URL \
  --supabase-key=$SUPABASE_KEY \
  --rds-endpoint=$RDS_ENDPOINT \
  --rds-user=$DB_USER \
  --rds-password=$DB_PASSWORD
```

#### 2.3 Migration DynamoDB (Haute FrÃ©quence)

```bash
npx tsx scripts/migrate_signals_to_dynamodb.ts \
  --batch-size=1000 \
  --table-name=personamy-prod-signals
```

---

### Phase 3 : Refactoring Code (Semaine 3-4)

#### 3.1 CrÃ©er Abstraction Layer

**Nouveau fichier : `services/api/src/db/index.ts`**
```typescript
// Abstraction pour masquer la source de donnÃ©es
export interface DatabaseClient {
  // Companies
  getCompanyByTicker(ticker: string): Promise<Company | null>;
  getCompanyByCik(cik: string): Promise<Company | null>;
  
  // Funds
  getFundById(id: number): Promise<Fund | null>;
  getFundByCik(cik: string): Promise<Fund | null>;
  
  // Holdings (depuis Athena)
  getHoldingsByFiling(filingId: number): Promise<Holding[]>;
  getHoldingsByFund(fundId: number, period?: string): Promise<Holding[]>;
  
  // Signals (depuis DynamoDB)
  getSignals(filters: SignalFilters): Promise<Signal[]>;
  createSignal(signal: SignalInput): Promise<Signal>;
}

// ImplÃ©mentation RDS pour companies/funds
export class RDSClient implements DatabaseClient {
  // Utilise pg (node-postgres) pour RDS
}

// ImplÃ©mentation Athena pour holdings
export class AthenaClient {
  // Utilise @aws-sdk/client-athena
}

// ImplÃ©mentation DynamoDB pour signals
export class DynamoDBClient {
  // Utilise @aws-sdk/client-dynamodb
}
```

#### 3.2 Migrer Services API

**Avant (Supabase) :**
```typescript
// services/api/src/companies.ts
import { supabase } from '../supabase';

export async function getCompanyByTicker(ticker: string) {
  const { data } = await supabase
    .from('companies')
    .select('*')
    .eq('ticker', ticker)
    .single();
  return data;
}
```

**AprÃ¨s (RDS) :**
```typescript
// services/api/src/companies.ts
import { db } from '../db';

export async function getCompanyByTicker(ticker: string) {
  return await db.getCompanyByTicker(ticker);
}
```

#### 3.3 Migrer Workers

**Parser 13F :**
- Avant : Insert `fund_holdings` dans Supabase
- AprÃ¨s : 
  1. Parser XML â†’ gÃ©nÃ©rer Parquet
  2. Upload vers S3 (partitionnÃ© par date)
  3. Refresh table Athena (`MSCK REPAIR TABLE`)

**Collectors (RSS, SEC) :**
- Avant : Insert `signals` dans Supabase
- AprÃ¨s : Insert dans DynamoDB

---

### Phase 4 : Tests & Validation (Semaine 5)

#### 4.1 Tests de Migration
- [ ] **VÃ©rifier intÃ©gritÃ© des donnÃ©es**
  ```bash
  # Comparer counts Supabase vs AWS
  npx tsx scripts/validate_migration.ts
  ```

- [ ] **Tests de performance**
  - Comparer latence API avant/aprÃ¨s
  - Tester requÃªtes Athena (holdings)
  - Tester requÃªtes RDS (companies)

#### 4.2 Tests d'IntÃ©gration
- [ ] **Tester endpoints API**
  - `/funds/{id}/holdings` (depuis Athena)
  - `/companies/ticker/{ticker}` (depuis RDS)
  - `/signals` (depuis DynamoDB)

- [ ] **Tester workers**
  - Parser 13F â†’ S3
  - Collectors â†’ DynamoDB

---

### Phase 5 : DÃ©ploiement & Cutover (Semaine 6)

#### 5.1 DÃ©ploiement Progressif
1. **DÃ©ployer infrastructure Terraform**
   ```bash
   cd infra/terraform
   terraform plan
   terraform apply
   ```

2. **Migrer les donnÃ©es** (voir Phase 2)

3. **DÃ©ployer code refactorisÃ©** (voir Phase 3)

4. **Basculer progressivement**
   - Semaine 1 : 10% du trafic vers AWS
   - Semaine 2 : 50% du trafic
   - Semaine 3 : 100% du trafic

#### 5.2 Monitoring
- [ ] **CloudWatch Dashboards**
  - Latence API
  - Erreurs Lambda
  - CoÃ»ts AWS (RDS, S3, Athena, DynamoDB)

- [ ] **Alertes**
  - RDS CPU > 80%
  - Lambda errors > 1%
  - CoÃ»ts > seuil

---

## ğŸ’° Estimation des CoÃ»ts AWS

### CoÃ»ts Mensuels EstimÃ©s

| Service | Configuration | CoÃ»t/mois |
|---------|--------------|-----------|
| **RDS PostgreSQL** | db.t3.micro (20GB) | $15 |
| **S3 Storage** | 10GB (Parquet compressÃ©) | $0.23 |
| **S3 Requests** | PUT/GET (1M requests) | $0.05 |
| **Athena** | 100GB scanned/mois | $5 |
| **DynamoDB** | PAY_PER_REQUEST (1M writes) | $1.25 |
| **Lambda** | InchangÃ© (dÃ©jÃ  en place) | $0 |
| **Data Transfer** | 10GB out | $0.90 |
| **TOTAL** | | **~$22.50/mois** |

### Comparaison Supabase vs AWS

| | Supabase PRO | AWS (estimÃ©) |
|---|---|---|
| **CoÃ»t base** | $25/mois | $22.50/mois |
| **Stockage** | 8GB inclus | 20GB RDS + 10GB S3 |
| **ScalabilitÃ©** | Limite 8GB | IllimitÃ©e (S3) |
| **Performance** | Bonne | Excellente (Athena pour analytics) |
| **FlexibilitÃ©** | LimitÃ©e | Totale (AWS native) |

**Ã‰conomie estimÃ©e** : ~$2.50/mois + scalabilitÃ© illimitÃ©e

---

## ğŸš¨ Risques & Mitigation

### Risque 1 : Downtime pendant migration
**Mitigation** :
- Migration en parallÃ¨le (Supabase + AWS)
- Basculer progressivement (10% â†’ 50% â†’ 100%)
- Rollback possible vers Supabase

### Risque 2 : Perte de donnÃ©es
**Mitigation** :
- Scripts de validation (counts, checksums)
- Snapshots RDS avant migration
- Backup S3 versionnÃ©

### Risque 3 : Latence Athena
**Mitigation** :
- Cache CloudFront pour requÃªtes frÃ©quentes
- Partitionnement optimal (year/month/day)
- Compression Parquet (Snappy)

### Risque 4 : CoÃ»ts AWS imprÃ©vus
**Mitigation** :
- Budgets AWS avec alertes
- Monitoring CloudWatch
- Optimisation continue (S3 lifecycle, RDS reserved instances)

---

## ğŸ“ Checklist ComplÃ¨te

### Infrastructure
- [ ] CrÃ©er RDS PostgreSQL (Terraform)
- [ ] CrÃ©er S3 buckets (data-lake, athena-results)
- [ ] CrÃ©er tables Athena (schemas Parquet)
- [ ] CrÃ©er tables DynamoDB
- [ ] Configurer VPC, Security Groups
- [ ] Configurer IAM roles/policies

### Migration DonnÃ©es
- [ ] Exporter `fund_holdings` â†’ S3 Parquet
- [ ] Exporter `company_filings` â†’ S3 Parquet
- [ ] Exporter `companies` â†’ RDS
- [ ] Exporter `funds` â†’ RDS
- [ ] Exporter `signals` â†’ DynamoDB
- [ ] Exporter `notifications` â†’ DynamoDB
- [ ] Valider intÃ©gritÃ© (counts, samples)

### Code
- [ ] CrÃ©er abstraction layer (`db/index.ts`)
- [ ] Migrer `services/api/src/companies.ts`
- [ ] Migrer `services/api/src/funds.ts`
- [ ] Migrer `services/api/src/holdings.ts`
- [ ] Migrer `workers/parser-13f` â†’ S3
- [ ] Migrer `workers/collector-*` â†’ DynamoDB
- [ ] Mettre Ã  jour tests

### Tests
- [ ] Tests unitaires (abstraction layer)
- [ ] Tests d'intÃ©gration (endpoints API)
- [ ] Tests de performance (latence)
- [ ] Tests de charge (Lambda concurrency)

### DÃ©ploiement
- [ ] DÃ©ployer infrastructure Terraform
- [ ] Migrer donnÃ©es
- [ ] DÃ©ployer code refactorisÃ©
- [ ] Basculer trafic progressivement
- [ ] Monitorer CloudWatch
- [ ] DÃ©sactiver Supabase (aprÃ¨s validation)

---

## ğŸ¯ Prochaines Ã‰tapes ImmÃ©diates

1. **CrÃ©er script d'audit Supabase**
   ```bash
   npx tsx scripts/audit_supabase_tables.ts
   ```

2. **CrÃ©er Terraform pour RDS**
   ```bash
   # CrÃ©er infra/terraform/rds.tf
   ```

3. **CrÃ©er script de migration holdings â†’ S3**
   ```bash
   # CrÃ©er scripts/migrate_holdings_to_s3.ts
   ```

4. **Tester migration sur un subset**
   ```bash
   # Migrer 10K holdings pour tester
   npx tsx scripts/migrate_holdings_to_s3.ts --limit=10000
   ```

---

## ğŸ“š Ressources

- [AWS RDS Pricing](https://aws.amazon.com/rds/pricing/)
- [AWS S3 Pricing](https://aws.amazon.com/s3/pricing/)
- [AWS Athena Pricing](https://aws.amazon.com/athena/pricing/)
- [AWS DynamoDB Pricing](https://aws.amazon.com/dynamodb/pricing/)
- [Parquet Format](https://parquet.apache.org/)
- [Athena Partitioning](https://docs.aws.amazon.com/athena/latest/ug/partitions.html)
